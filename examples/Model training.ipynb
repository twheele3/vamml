{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d49663-1d6c-4692-863d-eb3e49840520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 19:16:51.506667: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from vamml.Model import Model,Augments\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17356266-da40-4a54-b086-3f336624eb3c",
   "metadata": {},
   "source": [
    "Building a model with default parameters, which are set to the specs used in article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cf82b05-4914-42c4-ad88-5fbef4a5a18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 19:16:57.214255: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-15 19:16:58.864462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46264 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:18:00.0, compute capability: 8.6\n",
      "2025-09-15 19:16:58.871946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46595 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:c3:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = Model.BuildModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=10**-4),\n",
    "              loss='mse',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9919fd4-8b22-4525-ad88-fb9fe53b3654",
   "metadata": {},
   "source": [
    "Loading prepared tensorflow dataset compiled from experimental batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da23b52-2a17-4154-9788-2ad5ef17a234",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = tfds.builder_from_directory(builder_dir='./data/VAMML_dataset/')\n",
    "train_ds,test_ds = builder.as_dataset(split = ['train[:85%]', 'train[85%:]'],\n",
    "                                      shuffle_files = True,\n",
    "                                      batch_size = 32,\n",
    "                                      read_config = tfds.ReadConfig(shuffle_seed = 101),\n",
    "                                      as_supervised = True,\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede0bd36-137e-4087-8154-c8541c6f1354",
   "metadata": {},
   "source": [
    "Mapping training dataset with augmentations. This is a customized function that applies not only standard augmentations (flips, rotations, zoom), but a custom augmentation specific to this dataset. This augmentation takes the dose value (first position in dense inputs) and output image intensity, then randomly scales them inversely. ie, if the light dose and pixel intensity were initially 100 and 255, respectively, after augmentation it might be 125 and 204. This augmentation is designed to make the model recognize the correlation between voxel intensity and light dose so that it can scale voxel intensity without needing a well-predicted input intensity on the part of the operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8db8a8c0-5b78-4935-b499-a404da7d3858",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(Augments.vamml_augments,\n",
    "              num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "train_ds = train_ds.repeat()\n",
    "test_ds = test_ds.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77f8da11-b287-4cb9-b384-6557618f60ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 19:17:08.535516: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8400\n",
      "2025-09-15 19:17:08.844284: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 20s 280ms/step - loss: 0.2367 - val_loss: 0.2443\n",
      "Epoch 2/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.2076 - val_loss: 0.2358\n",
      "Epoch 3/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.1855 - val_loss: 0.2269\n",
      "Epoch 4/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.1703 - val_loss: 0.2163\n",
      "Epoch 5/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.1572 - val_loss: 0.2031\n",
      "Epoch 6/253\n",
      "20/20 [==============================] - 3s 129ms/step - loss: 0.1494 - val_loss: 0.1856\n",
      "Epoch 7/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.1406 - val_loss: 0.1652\n",
      "Epoch 8/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.1303 - val_loss: 0.1485\n",
      "Epoch 9/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.1201 - val_loss: 0.1350\n",
      "Epoch 10/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.1217 - val_loss: 0.1238\n",
      "Epoch 11/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.1131 - val_loss: 0.1142\n",
      "Epoch 12/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.1109 - val_loss: 0.1056\n",
      "Epoch 13/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0996 - val_loss: 0.0979\n",
      "Epoch 14/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.1066 - val_loss: 0.0914\n",
      "Epoch 15/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.1024 - val_loss: 0.0859\n",
      "Epoch 16/253\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.1012 - val_loss: 0.0810\n",
      "Epoch 17/253\n",
      "20/20 [==============================] - 3s 135ms/step - loss: 0.0978 - val_loss: 0.0771\n",
      "Epoch 18/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0863 - val_loss: 0.0731\n",
      "Epoch 19/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0898 - val_loss: 0.0700\n",
      "Epoch 20/253\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.0881 - val_loss: 0.0676\n",
      "Epoch 21/253\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.0960 - val_loss: 0.0660\n",
      "Epoch 22/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0916 - val_loss: 0.0642\n",
      "Epoch 23/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0830 - val_loss: 0.0626\n",
      "Epoch 24/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0803 - val_loss: 0.0611\n",
      "Epoch 25/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0818 - val_loss: 0.0598\n",
      "Epoch 26/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0687 - val_loss: 0.0587\n",
      "Epoch 27/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0907 - val_loss: 0.0578\n",
      "Epoch 28/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0860 - val_loss: 0.0571\n",
      "Epoch 29/253\n",
      "20/20 [==============================] - 3s 129ms/step - loss: 0.0701 - val_loss: 0.0564\n",
      "Epoch 30/253\n",
      "20/20 [==============================] - 3s 141ms/step - loss: 0.0853 - val_loss: 0.0558\n",
      "Epoch 31/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0729 - val_loss: 0.0552\n",
      "Epoch 32/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0751 - val_loss: 0.0548\n",
      "Epoch 33/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0694 - val_loss: 0.0544\n",
      "Epoch 34/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0693 - val_loss: 0.0539\n",
      "Epoch 35/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0731 - val_loss: 0.0536\n",
      "Epoch 36/253\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.0677 - val_loss: 0.0533\n",
      "Epoch 37/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0737 - val_loss: 0.0528\n",
      "Epoch 38/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0734 - val_loss: 0.0525\n",
      "Epoch 39/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0697 - val_loss: 0.0523\n",
      "Epoch 40/253\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.0779 - val_loss: 0.0520\n",
      "Epoch 41/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0798 - val_loss: 0.0515\n",
      "Epoch 42/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0681 - val_loss: 0.0515\n",
      "Epoch 43/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0729 - val_loss: 0.0516\n",
      "Epoch 44/253\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.0609 - val_loss: 0.0516\n",
      "Epoch 45/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0628 - val_loss: 0.0514\n",
      "Epoch 46/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0602 - val_loss: 0.0510\n",
      "Epoch 47/253\n",
      "20/20 [==============================] - 3s 135ms/step - loss: 0.0651 - val_loss: 0.0503\n",
      "Epoch 48/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0667 - val_loss: 0.0500\n",
      "Epoch 49/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0598 - val_loss: 0.0496\n",
      "Epoch 50/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0644 - val_loss: 0.0492\n",
      "Epoch 51/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0616 - val_loss: 0.0487\n",
      "Epoch 52/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0623 - val_loss: 0.0493\n",
      "Epoch 53/253\n",
      "20/20 [==============================] - 3s 129ms/step - loss: 0.0718 - val_loss: 0.0481\n",
      "Epoch 54/253\n",
      "20/20 [==============================] - 3s 129ms/step - loss: 0.0621 - val_loss: 0.0495\n",
      "Epoch 55/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0600 - val_loss: 0.0486\n",
      "Epoch 56/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0608 - val_loss: 0.0476\n",
      "Epoch 57/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0628 - val_loss: 0.0466\n",
      "Epoch 58/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0569 - val_loss: 0.0461\n",
      "Epoch 59/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0640 - val_loss: 0.0495\n",
      "Epoch 60/253\n",
      "20/20 [==============================] - 3s 129ms/step - loss: 0.0551 - val_loss: 0.0468\n",
      "Epoch 61/253\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.0618 - val_loss: 0.0450\n",
      "Epoch 62/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0628 - val_loss: 0.0434\n",
      "Epoch 63/253\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.0573 - val_loss: 0.0426\n",
      "Epoch 64/253\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.0538 - val_loss: 0.0426\n",
      "Epoch 65/253\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.0581 - val_loss: 0.0409\n",
      "Epoch 66/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0536 - val_loss: 0.0412\n",
      "Epoch 67/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0633 - val_loss: 0.0392\n",
      "Epoch 68/253\n",
      "20/20 [==============================] - 3s 137ms/step - loss: 0.0562 - val_loss: 0.0385\n",
      "Epoch 69/253\n",
      "20/20 [==============================] - 3s 129ms/step - loss: 0.0533 - val_loss: 0.0387\n",
      "Epoch 70/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0528 - val_loss: 0.0397\n",
      "Epoch 71/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0524 - val_loss: 0.0382\n",
      "Epoch 72/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0598 - val_loss: 0.0374\n",
      "Epoch 73/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0611 - val_loss: 0.0374\n",
      "Epoch 74/253\n",
      "20/20 [==============================] - 3s 135ms/step - loss: 0.0485 - val_loss: 0.0364\n",
      "Epoch 75/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0571 - val_loss: 0.0356\n",
      "Epoch 76/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0547 - val_loss: 0.0354\n",
      "Epoch 77/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0481 - val_loss: 0.0348\n",
      "Epoch 78/253\n",
      "20/20 [==============================] - 3s 136ms/step - loss: 0.0538 - val_loss: 0.0351\n",
      "Epoch 79/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0506 - val_loss: 0.0350\n",
      "Epoch 80/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0508 - val_loss: 0.0336\n",
      "Epoch 81/253\n",
      "20/20 [==============================] - 3s 135ms/step - loss: 0.0522 - val_loss: 0.0335\n",
      "Epoch 82/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0625 - val_loss: 0.0336\n",
      "Epoch 83/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0627 - val_loss: 0.0343\n",
      "Epoch 84/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0446 - val_loss: 0.0330\n",
      "Epoch 85/253\n",
      "20/20 [==============================] - 3s 135ms/step - loss: 0.0500 - val_loss: 0.0327\n",
      "Epoch 86/253\n",
      "20/20 [==============================] - 3s 136ms/step - loss: 0.0460 - val_loss: 0.0329\n",
      "Epoch 87/253\n",
      "20/20 [==============================] - 3s 136ms/step - loss: 0.0559 - val_loss: 0.0339\n",
      "Epoch 88/253\n",
      "20/20 [==============================] - 3s 135ms/step - loss: 0.0452 - val_loss: 0.0343\n",
      "Epoch 89/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0557 - val_loss: 0.0335\n",
      "Epoch 90/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0532 - val_loss: 0.0336\n",
      "Epoch 91/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0530 - val_loss: 0.0333\n",
      "Epoch 92/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0462 - val_loss: 0.0329\n",
      "Epoch 93/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0468 - val_loss: 0.0324\n",
      "Epoch 94/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0437 - val_loss: 0.0320\n",
      "Epoch 95/253\n",
      "20/20 [==============================] - 3s 128ms/step - loss: 0.0513 - val_loss: 0.0319\n",
      "Epoch 96/253\n",
      "20/20 [==============================] - 3s 127ms/step - loss: 0.0502 - val_loss: 0.0320\n",
      "Epoch 97/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0397 - val_loss: 0.0333\n",
      "Epoch 98/253\n",
      "20/20 [==============================] - 3s 129ms/step - loss: 0.0520 - val_loss: 0.0319\n",
      "Epoch 99/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0533 - val_loss: 0.0316\n",
      "Epoch 100/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0494 - val_loss: 0.0324\n",
      "Epoch 101/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0414 - val_loss: 0.0325\n",
      "Epoch 102/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0398 - val_loss: 0.0316\n",
      "Epoch 103/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0444 - val_loss: 0.0335\n",
      "Epoch 104/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0420 - val_loss: 0.0329\n",
      "Epoch 105/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0469 - val_loss: 0.0303\n",
      "Epoch 106/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0455 - val_loss: 0.0304\n",
      "Epoch 107/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0444 - val_loss: 0.0302\n",
      "Epoch 108/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0462 - val_loss: 0.0299\n",
      "Epoch 109/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0474 - val_loss: 0.0292\n",
      "Epoch 110/253\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.0472 - val_loss: 0.0302\n",
      "Epoch 111/253\n",
      "20/20 [==============================] - 3s 136ms/step - loss: 0.0451 - val_loss: 0.0305\n",
      "Epoch 112/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0496 - val_loss: 0.0299\n",
      "Epoch 113/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0425 - val_loss: 0.0307\n",
      "Epoch 114/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0428 - val_loss: 0.0305\n",
      "Epoch 115/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0462 - val_loss: 0.0289\n",
      "Epoch 116/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0460 - val_loss: 0.0296\n",
      "Epoch 117/253\n",
      "20/20 [==============================] - 3s 128ms/step - loss: 0.0449 - val_loss: 0.0304\n",
      "Epoch 118/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0444 - val_loss: 0.0299\n",
      "Epoch 119/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0463 - val_loss: 0.0298\n",
      "Epoch 120/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0476 - val_loss: 0.0296\n",
      "Epoch 121/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0413 - val_loss: 0.0288\n",
      "Epoch 122/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0373 - val_loss: 0.0276\n",
      "Epoch 123/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0375 - val_loss: 0.0281\n",
      "Epoch 124/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0440 - val_loss: 0.0273\n",
      "Epoch 125/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0483 - val_loss: 0.0276\n",
      "Epoch 126/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0374 - val_loss: 0.0276\n",
      "Epoch 127/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0369 - val_loss: 0.0270\n",
      "Epoch 128/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0474 - val_loss: 0.0268\n",
      "Epoch 129/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0406 - val_loss: 0.0309\n",
      "Epoch 130/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0491 - val_loss: 0.0305\n",
      "Epoch 131/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0422 - val_loss: 0.0298\n",
      "Epoch 132/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0392 - val_loss: 0.0296\n",
      "Epoch 133/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0400 - val_loss: 0.0278\n",
      "Epoch 134/253\n",
      "20/20 [==============================] - 3s 136ms/step - loss: 0.0495 - val_loss: 0.0281\n",
      "Epoch 135/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0451 - val_loss: 0.0284\n",
      "Epoch 136/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0391 - val_loss: 0.0284\n",
      "Epoch 137/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0441 - val_loss: 0.0276\n",
      "Epoch 138/253\n",
      "20/20 [==============================] - 3s 129ms/step - loss: 0.0340 - val_loss: 0.0278\n",
      "Epoch 139/253\n",
      "20/20 [==============================] - 2s 126ms/step - loss: 0.0357 - val_loss: 0.0264\n",
      "Epoch 140/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0426 - val_loss: 0.0258\n",
      "Epoch 141/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0355 - val_loss: 0.0255\n",
      "Epoch 142/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0390 - val_loss: 0.0247\n",
      "Epoch 143/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0374 - val_loss: 0.0252\n",
      "Epoch 144/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0378 - val_loss: 0.0250\n",
      "Epoch 145/253\n",
      "20/20 [==============================] - 3s 129ms/step - loss: 0.0384 - val_loss: 0.0251\n",
      "Epoch 146/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0395 - val_loss: 0.0252\n",
      "Epoch 147/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0351 - val_loss: 0.0254\n",
      "Epoch 148/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0432 - val_loss: 0.0255\n",
      "Epoch 149/253\n",
      "20/20 [==============================] - 3s 129ms/step - loss: 0.0335 - val_loss: 0.0253\n",
      "Epoch 150/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0384 - val_loss: 0.0256\n",
      "Epoch 151/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0397 - val_loss: 0.0258\n",
      "Epoch 152/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0382 - val_loss: 0.0254\n",
      "Epoch 153/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0393 - val_loss: 0.0254\n",
      "Epoch 154/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0395 - val_loss: 0.0261\n",
      "Epoch 155/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0372 - val_loss: 0.0259\n",
      "Epoch 156/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0369 - val_loss: 0.0255\n",
      "Epoch 157/253\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.0385 - val_loss: 0.0248\n",
      "Epoch 158/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0404 - val_loss: 0.0251\n",
      "Epoch 159/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0327 - val_loss: 0.0249\n",
      "Epoch 160/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0421 - val_loss: 0.0242\n",
      "Epoch 161/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0352 - val_loss: 0.0243\n",
      "Epoch 162/253\n",
      "20/20 [==============================] - 3s 135ms/step - loss: 0.0331 - val_loss: 0.0253\n",
      "Epoch 163/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0345 - val_loss: 0.0241\n",
      "Epoch 164/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0356 - val_loss: 0.0242\n",
      "Epoch 165/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0394 - val_loss: 0.0249\n",
      "Epoch 166/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0340 - val_loss: 0.0245\n",
      "Epoch 167/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0366 - val_loss: 0.0243\n",
      "Epoch 168/253\n",
      "20/20 [==============================] - 3s 137ms/step - loss: 0.0393 - val_loss: 0.0238\n",
      "Epoch 169/253\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.0347 - val_loss: 0.0244\n",
      "Epoch 170/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0353 - val_loss: 0.0244\n",
      "Epoch 171/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0346 - val_loss: 0.0239\n",
      "Epoch 172/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0377 - val_loss: 0.0240\n",
      "Epoch 173/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0328 - val_loss: 0.0233\n",
      "Epoch 174/253\n",
      "20/20 [==============================] - 3s 129ms/step - loss: 0.0323 - val_loss: 0.0234\n",
      "Epoch 175/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0391 - val_loss: 0.0233\n",
      "Epoch 176/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0411 - val_loss: 0.0247\n",
      "Epoch 177/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0330 - val_loss: 0.0242\n",
      "Epoch 178/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0345 - val_loss: 0.0238\n",
      "Epoch 179/253\n",
      "20/20 [==============================] - 3s 129ms/step - loss: 0.0339 - val_loss: 0.0237\n",
      "Epoch 180/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0376 - val_loss: 0.0240\n",
      "Epoch 181/253\n",
      "20/20 [==============================] - 3s 127ms/step - loss: 0.0367 - val_loss: 0.0236\n",
      "Epoch 182/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0314 - val_loss: 0.0230\n",
      "Epoch 183/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0341 - val_loss: 0.0242\n",
      "Epoch 184/253\n",
      "20/20 [==============================] - 3s 129ms/step - loss: 0.0328 - val_loss: 0.0241\n",
      "Epoch 185/253\n",
      "20/20 [==============================] - 3s 135ms/step - loss: 0.0328 - val_loss: 0.0239\n",
      "Epoch 186/253\n",
      "20/20 [==============================] - 3s 135ms/step - loss: 0.0370 - val_loss: 0.0226\n",
      "Epoch 187/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0307 - val_loss: 0.0227\n",
      "Epoch 188/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0341 - val_loss: 0.0229\n",
      "Epoch 189/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0358 - val_loss: 0.0233\n",
      "Epoch 190/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0351 - val_loss: 0.0236\n",
      "Epoch 191/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0314 - val_loss: 0.0240\n",
      "Epoch 192/253\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.0369 - val_loss: 0.0231\n",
      "Epoch 193/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0389 - val_loss: 0.0224\n",
      "Epoch 194/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0386 - val_loss: 0.0232\n",
      "Epoch 195/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0348 - val_loss: 0.0229\n",
      "Epoch 196/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0294 - val_loss: 0.0231\n",
      "Epoch 197/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0327 - val_loss: 0.0223\n",
      "Epoch 198/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0336 - val_loss: 0.0227\n",
      "Epoch 199/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0372 - val_loss: 0.0235\n",
      "Epoch 200/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0351 - val_loss: 0.0240\n",
      "Epoch 201/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0291 - val_loss: 0.0238\n",
      "Epoch 202/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0333 - val_loss: 0.0236\n",
      "Epoch 203/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0356 - val_loss: 0.0238\n",
      "Epoch 204/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0366 - val_loss: 0.0238\n",
      "Epoch 205/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0348 - val_loss: 0.0238\n",
      "Epoch 206/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0348 - val_loss: 0.0238\n",
      "Epoch 207/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0329 - val_loss: 0.0238\n",
      "Epoch 208/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0349 - val_loss: 0.0234\n",
      "Epoch 209/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0333 - val_loss: 0.0236\n",
      "Epoch 210/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0370 - val_loss: 0.0241\n",
      "Epoch 211/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0327 - val_loss: 0.0253\n",
      "Epoch 212/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0352 - val_loss: 0.0255\n",
      "Epoch 213/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0334 - val_loss: 0.0245\n",
      "Epoch 214/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0316 - val_loss: 0.0238\n",
      "Epoch 215/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0298 - val_loss: 0.0231\n",
      "Epoch 216/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0324 - val_loss: 0.0236\n",
      "Epoch 217/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0329 - val_loss: 0.0233\n",
      "Epoch 218/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0339 - val_loss: 0.0244\n",
      "Epoch 219/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0307 - val_loss: 0.0235\n",
      "Epoch 220/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0327 - val_loss: 0.0229\n",
      "Epoch 221/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0294 - val_loss: 0.0237\n",
      "Epoch 222/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0339 - val_loss: 0.0238\n",
      "Epoch 223/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0290 - val_loss: 0.0231\n",
      "Epoch 224/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0307 - val_loss: 0.0223\n",
      "Epoch 225/253\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.0324 - val_loss: 0.0224\n",
      "Epoch 226/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0292 - val_loss: 0.0225\n",
      "Epoch 227/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0314 - val_loss: 0.0224\n",
      "Epoch 228/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0312 - val_loss: 0.0226\n",
      "Epoch 229/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0325 - val_loss: 0.0222\n",
      "Epoch 230/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0302 - val_loss: 0.0223\n",
      "Epoch 231/253\n",
      "20/20 [==============================] - 3s 129ms/step - loss: 0.0338 - val_loss: 0.0227\n",
      "Epoch 232/253\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.0299 - val_loss: 0.0223\n",
      "Epoch 233/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0313 - val_loss: 0.0223\n",
      "Epoch 234/253\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0299 - val_loss: 0.0221\n",
      "Epoch 235/253\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.0320 - val_loss: 0.0223\n",
      "Epoch 236/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0325 - val_loss: 0.0230\n",
      "Epoch 237/253\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.0316 - val_loss: 0.0231\n",
      "Epoch 238/253\n",
      "20/20 [==============================] - 3s 136ms/step - loss: 0.0337 - val_loss: 0.0231\n",
      "Epoch 239/253\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.0313 - val_loss: 0.0236\n",
      "Epoch 240/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0293 - val_loss: 0.0235\n",
      "Epoch 241/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0304 - val_loss: 0.0230\n",
      "Epoch 242/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0320 - val_loss: 0.0233\n",
      "Epoch 243/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0307 - val_loss: 0.0239\n",
      "Epoch 244/253\n",
      "20/20 [==============================] - 3s 136ms/step - loss: 0.0289 - val_loss: 0.0242\n",
      "Epoch 245/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0309 - val_loss: 0.0229\n",
      "Epoch 246/253\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.0317 - val_loss: 0.0240\n",
      "Epoch 247/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0345 - val_loss: 0.0249\n",
      "Epoch 248/253\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0312 - val_loss: 0.0276\n",
      "Epoch 249/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0309 - val_loss: 0.0264\n",
      "Epoch 250/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0281 - val_loss: 0.0260\n",
      "Epoch 251/253\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0290 - val_loss: 0.0238\n",
      "Epoch 252/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0337 - val_loss: 0.0235\n",
      "Epoch 253/253\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.0302 - val_loss: 0.0238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7a2c8d3ee370>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, \n",
    "          validation_data=test_ds,\n",
    "          steps_per_epoch=20, \n",
    "          epochs=253,\n",
    "          validation_steps = 1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4c42a03-543c-4883-8143-c9968b0d870e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image (InputLayer)             [(None, 256, 256, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d0_1 (Conv2D)             (None, 256, 256, 8)  80          ['image[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 256, 256, 8)  32         ['conv2d0_1[0][0]']              \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 256, 256, 8)  0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout0 (Dropout)             (None, 256, 256, 8)  0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d0_2 (Conv2D)             (None, 256, 256, 8)  584         ['dropout0[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 256, 256, 8)  32         ['conv2d0_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 256, 256, 8)  0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " maxpool1 (MaxPooling2D)        (None, 128, 128, 8)  0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d1_1 (Conv2D)             (None, 128, 128, 16  1168        ['maxpool1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 128, 128, 16  64         ['conv2d1_1[0][0]']              \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 128, 128, 16  0           ['batch_normalization_2[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout1 (Dropout)             (None, 128, 128, 16  0           ['activation_2[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d1_2 (Conv2D)             (None, 128, 128, 16  2320        ['dropout1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 128, 128, 16  64         ['conv2d1_2[0][0]']              \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 128, 128, 16  0           ['batch_normalization_3[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " maxpool2 (MaxPooling2D)        (None, 64, 64, 16)   0           ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d2_1 (Conv2D)             (None, 64, 64, 32)   4640        ['maxpool2[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 64, 64, 32)  128         ['conv2d2_1[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 64, 64, 32)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout2 (Dropout)             (None, 64, 64, 32)   0           ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d2_2 (Conv2D)             (None, 64, 64, 32)   9248        ['dropout2[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 64, 64, 32)  128         ['conv2d2_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 64, 64, 32)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " maxpool3 (MaxPooling2D)        (None, 32, 32, 32)   0           ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d3_1 (Conv2D)             (None, 32, 32, 64)   18496       ['maxpool3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 64)  256         ['conv2d3_1[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 64)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout3 (Dropout)             (None, 32, 32, 64)   0           ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d3_2 (Conv2D)             (None, 32, 32, 64)   36928       ['dropout3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 64)  256         ['conv2d3_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 32, 32, 64)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " maxpool4 (MaxPooling2D)        (None, 16, 16, 64)   0           ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d4_1 (Conv2D)             (None, 16, 16, 128)  73856       ['maxpool4[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 16, 16, 128)  512        ['conv2d4_1[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 16, 16, 128)  0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dropout4 (Dropout)             (None, 16, 16, 128)  0           ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " metadata (InputLayer)          [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " conv2d4_2 (Conv2D)             (None, 16, 16, 128)  147584      ['dropout4[0][0]']               \n",
      "                                                                                                  \n",
      " token_bn (BatchNormalization)  (None, 6)            24          ['metadata[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 16, 16, 128)  512        ['conv2d4_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense0 (Dense)                 (None, 128)          896         ['token_bn[0][0]']               \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 16, 16, 128)  0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (None, 1, 128)       0           ['dense0[0][0]']                 \n",
      "                                                                                                  \n",
      " contractbottom (MaxPooling2D)  (None, 1, 1, 128)    0           ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_1 (TFOpLambda)  (None, 1, 1, 128)    0           ['tf.expand_dims[0][0]']         \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 1, 1, 256)    0           ['contractbottom[0][0]',         \n",
      "                                                                  'tf.expand_dims_1[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 1, 1, 256)   1024        ['tf.concat[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " bottom_dense_1 (Dense)         (None, 1, 1, 128)    32896       ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " upconv_attn_dropout4 (Dropout)  (None, 1, 1, 128)   0           ['bottom_dense_1[0][0]']         \n",
      "                                                                                                  \n",
      " bottom_dense_2 (Dense)         (None, 1, 1, 128)    16512       ['upconv_attn_dropout4[0][0]']   \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 1, 1, 128)    0           ['bottom_dense_2[0][0]']         \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 16, 16, 128)  0           ['activation_9[0][0]',           \n",
      "                                                                  'activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " upconv_transpose3 (Conv2DTrans  (None, 32, 32, 64)  73792       ['tf.math.multiply[0][0]']       \n",
      " pose)                                                                                            \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 64)  256         ['upconv_transpose3[0][0]']      \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 32, 32, 64)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " concat3 (Concatenate)          (None, 32, 32, 128)  0           ['activation_11[0][0]',          \n",
      "                                                                  'activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " upconv_pool3 (MaxPooling2D)    (None, 1, 1, 128)    0           ['concat3[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 1, 1, 128)   512         ['upconv_pool3[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.concat_1 (TFOpLambda)       (None, 1, 1, 256)    0           ['batch_normalization_12[0][0]', \n",
      "                                                                  'tf.expand_dims_1[0][0]']       \n",
      "                                                                                                  \n",
      " upconv_attn3_1 (Dense)         (None, 1, 1, 128)    32896       ['tf.concat_1[0][0]']            \n",
      "                                                                                                  \n",
      " upconv_attn_dropout3 (Dropout)  (None, 1, 1, 128)   0           ['upconv_attn3_1[0][0]']         \n",
      "                                                                                                  \n",
      " upconv_attn3_2 (Dense)         (None, 1, 1, 128)    16512       ['upconv_attn_dropout3[0][0]']   \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 1, 1, 128)    0           ['upconv_attn3_2[0][0]']         \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  (None, 32, 32, 128)  0          ['concat3[0][0]',                \n",
      " )                                                                'activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " upconv_2d3_1 (Conv2D)          (None, 32, 32, 64)   73792       ['tf.math.multiply_1[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 32, 32, 64)  256         ['upconv_2d3_1[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " upconv_dropout3 (Dropout)      (None, 32, 32, 64)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " upconv_2d3_2 (Conv2D)          (None, 32, 32, 64)   36928       ['upconv_dropout3[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 32, 32, 64)  256         ['upconv_2d3_2[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " upconv_transpose2 (Conv2DTrans  (None, 64, 64, 32)  18464       ['batch_normalization_14[0][0]'] \n",
      " pose)                                                                                            \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 64, 64, 32)  128         ['upconv_transpose2[0][0]']      \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 64, 64, 32)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " concat2 (Concatenate)          (None, 64, 64, 64)   0           ['activation_13[0][0]',          \n",
      "                                                                  'activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " upconv_pool2 (MaxPooling2D)    (None, 1, 1, 64)     0           ['concat2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 1, 1, 64)    256         ['upconv_pool2[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.concat_2 (TFOpLambda)       (None, 1, 1, 192)    0           ['batch_normalization_16[0][0]', \n",
      "                                                                  'tf.expand_dims_1[0][0]']       \n",
      "                                                                                                  \n",
      " upconv_attn2_1 (Dense)         (None, 1, 1, 64)     12352       ['tf.concat_2[0][0]']            \n",
      "                                                                                                  \n",
      " upconv_attn_dropout2 (Dropout)  (None, 1, 1, 64)    0           ['upconv_attn2_1[0][0]']         \n",
      "                                                                                                  \n",
      " upconv_attn2_2 (Dense)         (None, 1, 1, 64)     4160        ['upconv_attn_dropout2[0][0]']   \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 1, 1, 64)     0           ['upconv_attn2_2[0][0]']         \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLambda  (None, 64, 64, 64)  0           ['concat2[0][0]',                \n",
      " )                                                                'activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " upconv_2d2_1 (Conv2D)          (None, 64, 64, 32)   18464       ['tf.math.multiply_2[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 64, 64, 32)  128         ['upconv_2d2_1[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " upconv_dropout2 (Dropout)      (None, 64, 64, 32)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " upconv_2d2_2 (Conv2D)          (None, 64, 64, 32)   9248        ['upconv_dropout2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 64, 64, 32)  128         ['upconv_2d2_2[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " upconv_transpose1 (Conv2DTrans  (None, 128, 128, 16  4624       ['batch_normalization_18[0][0]'] \n",
      " pose)                          )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 128, 128, 16  64         ['upconv_transpose1[0][0]']      \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 128, 128, 16  0           ['batch_normalization_19[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concat1 (Concatenate)          (None, 128, 128, 32  0           ['activation_15[0][0]',          \n",
      "                                )                                 'activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " upconv_pool1 (MaxPooling2D)    (None, 1, 1, 32)     0           ['concat1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 1, 1, 32)    128         ['upconv_pool1[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.concat_3 (TFOpLambda)       (None, 1, 1, 160)    0           ['batch_normalization_20[0][0]', \n",
      "                                                                  'tf.expand_dims_1[0][0]']       \n",
      "                                                                                                  \n",
      " upconv_attn1_1 (Dense)         (None, 1, 1, 32)     5152        ['tf.concat_3[0][0]']            \n",
      "                                                                                                  \n",
      " upconv_attn_dropout1 (Dropout)  (None, 1, 1, 32)    0           ['upconv_attn1_1[0][0]']         \n",
      "                                                                                                  \n",
      " upconv_attn1_2 (Dense)         (None, 1, 1, 32)     1056        ['upconv_attn_dropout1[0][0]']   \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 1, 1, 32)     0           ['upconv_attn1_2[0][0]']         \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLambda  (None, 128, 128, 32  0          ['concat1[0][0]',                \n",
      " )                              )                                 'activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " upconv_2d1_1 (Conv2D)          (None, 128, 128, 16  4624        ['tf.math.multiply_3[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 128, 128, 16  64         ['upconv_2d1_1[0][0]']           \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " upconv_dropout1 (Dropout)      (None, 128, 128, 16  0           ['batch_normalization_21[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " upconv_2d1_2 (Conv2D)          (None, 128, 128, 16  2320        ['upconv_dropout1[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 128, 128, 16  64         ['upconv_2d1_2[0][0]']           \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " upconv_transpose0 (Conv2DTrans  (None, 256, 256, 8)  1160       ['batch_normalization_22[0][0]'] \n",
      " pose)                                                                                            \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 256, 256, 8)  32         ['upconv_transpose0[0][0]']      \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 256, 256, 8)  0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " concat0 (Concatenate)          (None, 256, 256, 16  0           ['activation_17[0][0]',          \n",
      "                                )                                 'activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " upconv_pool0 (MaxPooling2D)    (None, 1, 1, 16)     0           ['concat0[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 1, 1, 16)    64          ['upconv_pool0[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.concat_4 (TFOpLambda)       (None, 1, 1, 144)    0           ['batch_normalization_24[0][0]', \n",
      "                                                                  'tf.expand_dims_1[0][0]']       \n",
      "                                                                                                  \n",
      " upconv_attn0_1 (Dense)         (None, 1, 1, 16)     2320        ['tf.concat_4[0][0]']            \n",
      "                                                                                                  \n",
      " upconv_attn_dropout0 (Dropout)  (None, 1, 1, 16)    0           ['upconv_attn0_1[0][0]']         \n",
      "                                                                                                  \n",
      " upconv_attn0_2 (Dense)         (None, 1, 1, 16)     272         ['upconv_attn_dropout0[0][0]']   \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 1, 1, 16)     0           ['upconv_attn0_2[0][0]']         \n",
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLambda  (None, 256, 256, 16  0          ['concat0[0][0]',                \n",
      " )                              )                                 'activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " upconv_2d0_1 (Conv2D)          (None, 256, 256, 8)  1160        ['tf.math.multiply_4[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 256, 256, 8)  32         ['upconv_2d0_1[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " upconv_dropout0 (Dropout)      (None, 256, 256, 8)  0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " upconv_2d0_2 (Conv2D)          (None, 256, 256, 8)  584         ['upconv_dropout0[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 256, 256, 8)  32         ['upconv_2d0_2[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " output_shape (Conv2D)          (None, 256, 256, 1)  9           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 670,529\n",
      "Trainable params: 667,813\n",
      "Non-trainable params: 2,716\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
